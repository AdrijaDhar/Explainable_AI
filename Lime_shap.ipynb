{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: 'D:\\\\Explainable_AI-main\\\\arff_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m rows \u001b[39m=\u001b[39m []\n\u001b[0;32m     88\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m---> 89\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mExplainable_AI-main\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39marff_all\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m     90\u001b[0m   \u001b[39m#print(file)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m   \u001b[39mif\u001b[39;00m count \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     92\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'D:\\\\Explainable_AI-main\\\\arff_all'"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import flax\n",
    "# function to make dataframe from .arff file\n",
    "def load(f):\n",
    " with open(f, 'r') as fp:\n",
    "    try:\n",
    "      file_content = fp.readlines()\n",
    "    except:\n",
    "      return \"NOT READABLE\"\n",
    "\n",
    "\n",
    " def parse_row(line, len_row):\n",
    "    line = line.replace('{', '').replace('}', '')\n",
    "\n",
    "    row = np.zeros(len_row)\n",
    "    d=line.split(',')\n",
    "    if(len(d)!=1):\n",
    "     for data in line.split(','):\n",
    "         index, value = data.split()\n",
    "         row[int(index)] = float(value)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    " columns = []\n",
    " len_attr = len('@attribute')\n",
    " \n",
    "\n",
    "# get the columns\n",
    " for line in file_content:\n",
    "    if line.startswith('@attribute '):\n",
    "        col_name = line[len_attr:].split()[0]\n",
    "        columns.append(col_name)\n",
    " rows = []\n",
    " len_row = len(columns)\n",
    "# get the rows\n",
    " for line in file_content:\n",
    "    if line.startswith('{'):\n",
    "        \n",
    "        rows.append(parse_row(line, len_row))\n",
    "\n",
    " df = pd.DataFrame(data=rows, columns=columns)\n",
    " # give path of training (   ......Recon) or test file(/content/drive/MyDrive/arff_manual2/ ...our data)\n",
    " domain_os=f.replace(\"D:\\\\Explainable_AI-main\\\\arff_all\",\"\")\n",
    " domain_os=domain_os.replace(\".arff\",\"\")\n",
    " temp=domain_os.split(\"_\")\n",
    " domain=temp[0]\n",
    " os=temp[1]\n",
    " fo=[]\n",
    " fo1=[]\n",
    " for i in range(0,len(df.index)):\n",
    "   fo.append(domain)\n",
    "   fo1.append(os)\n",
    "\n",
    " df.insert(1,\"domain\",fo)\n",
    " df.insert(2,\"OS\",fo1)\n",
    "\n",
    " return df\n",
    "# making final data frame by combining all training dataframes (per domain)\n",
    "\n",
    "# Combing columns\n",
    "col = []\n",
    "coli = []\n",
    "rows = []\n",
    "count = 500\n",
    "for file in os.listdir(\"D:\\\\Explainable_AI-main\\\\arff_all\"):\n",
    "  #print(file)\n",
    "  if count == 0:\n",
    "    break\n",
    "  count = count - 1\n",
    "  if file == \"general_all.arff\":\n",
    "    continue\n",
    "\n",
    "  s = \"D:\\\\Explainable_AI-main\\\\arff_all\\\\\" + file\n",
    "  df1 = load(s)\n",
    "  if type(df1) != pd.core.frame.DataFrame:\n",
    "    continue\n",
    "\n",
    "  col1 = df1.columns\n",
    "  col = np.concatenate((col, col1))\n",
    "  if len(coli) == 0:\n",
    "    coli = col1\n",
    "  else:\n",
    "    coli = np.intersect1d(coli, col1)\n",
    "\n",
    "col = np.unique(col)\n",
    "# print(col)\n",
    "# print(coli)\n",
    "# print(len(col))\n",
    "\n",
    "\n",
    "# Combining rows\n",
    "count = 500\n",
    "for file in os.listdir(\"D:\\\\Explainable_AI-main\\\\arff_all\"):\n",
    "  if count == 0:\n",
    "    break\n",
    "  count = count - 1\n",
    "  if file == \"general_all.arff\":\n",
    "    continue\n",
    "  # print(file)\n",
    "  s = \"D:\\\\Explainable_AI-main\\\\arff_all\\\\\" + file\n",
    "  df1 = load(s)\n",
    "  if type(df1) != pd.core.frame.DataFrame:\n",
    "    continue\n",
    "  col1 = df1.columns\n",
    "  for index, row in df1.iterrows():\n",
    "    map = {}\n",
    "    for c in col1:\n",
    "      map[c] = row[c]\n",
    "\n",
    "    ro = []\n",
    "    for i in range(0, len(col)):\n",
    "      ro.append(\"0.0\")\n",
    "\n",
    "    for i in range(0, len(col)):\n",
    "      t = col[i]\n",
    "      if t in map:\n",
    "        ro[i] = map[t]\n",
    "\n",
    "    rows.append(ro)\n",
    " \n",
    "      \n",
    "   \n",
    "# Chnaging position for y column to last       \n",
    "df = pd.DataFrame(data=rows, columns=col)\n",
    "#df=df.fillna(0.0)\n",
    "col2=col.copy\n",
    "c=df.pop(\"PIILabel\")\n",
    "d=df.pop(\"domain\")\n",
    "e=df.pop(\"OS\")\n",
    "df.insert(len(df.columns), 'PIILabel', c)\n",
    "df.insert(0, 'domain', d)\n",
    "df.insert(1, 'OS', e)\n",
    "\n",
    "df\n",
    "np.unique(df['PIILabel'], return_counts = True)\n",
    "!mkdir combined_data_stuff\n",
    "imp_features = pd.read_csv(\"/media/rishika/Seagate Expansion Drive/shreyas_btp_pii_leakage/combined_data_stuff/imp_features_rp_heuristic.csv\").drop(\"Unnamed: 0\", axis = 1)\n",
    "df_wod = df.drop([\"domain\",\"OS\"], axis = 1)\n",
    "c_data = []\n",
    "for i in df_wod.columns:\n",
    "    no_pkts_1 = 0\n",
    "    no_pkts_pil = 0\n",
    "    for j in df_wod.index:\n",
    "        if(df_wod[i][j] != 0):\n",
    "            if(df_wod[\"PIILabel\"][j] != 0):\n",
    "                no_pkts_pil = no_pkts_pil + 1\n",
    "            no_pkts_1 = no_pkts_1 + 1\n",
    "    confidence = no_pkts_pil/no_pkts_1\n",
    "    c_data.append([i, confidence])\n",
    "c_df = pd.DataFrame(c_data)\n",
    "c_df.rename(columns = {0:'col_name', 1:'confidence_level'}, inplace = True)\n",
    "c_df_imp = c_df[c_df.confidence_level > 0.17]\n",
    "c_df_imp\n",
    "c_df_imp.to_csv(\"combined_data_stuff/imp_features_rp_heuristic.csv\")\n",
    "imp_cols = list(c_df_imp['col_name'])\n",
    "undesired_columns=[]\n",
    "for i in df_wod.columns:\n",
    "    if i not in imp_cols:\n",
    "        undesired_columns.append(i)\n",
    "heuristic_df= df_wod.drop(undesired_columns, axis=1)\n",
    "heuristic_df.shape\n",
    "heuristic_df.head()\n",
    "def convert_bin(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "bin_values = heuristic_df.values.astype('float32')\n",
    "conv_bin = np.vectorize(convert_bin)\n",
    "bin_values = conv_bin(bin_values)\n",
    "bin_df = pd.DataFrame(bin_values, columns = heuristic_df.columns)\n",
    "bin_df.to_csv(\"combined_data_stuff/bin_data_full.csv\")\n",
    "bin_df.head()\n",
    "bin_df = pd.read_csv('combined_data_stuff/bin_data_full.csv').drop(['Unnamed: 0'],axis=1)\n",
    "bin_df.head()\n",
    "X = bin_df.iloc[:, :-1]\n",
    "y = bin_df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "x_train_data, x_val_data, y_train_data, y_val_data = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "model=models.Sequential()\n",
    "model.add(layers.Dense(x_train_data.shape[1],activation='relu',input_shape=(x_train_data.shape[1],)))\n",
    "model.add(layers.Dense(1024,activation='relu'))\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "import time\n",
    "st = time.time()\n",
    "model.fit(x_train_data,y_train_data,epochs=5,batch_size=32,validation_data=(x_val_data,y_val_data))\n",
    "end = time.time()\n",
    "time_taken_nn_train = end-st\n",
    "print(\"Time taken to train NN = \", time_taken_nn_train, \"seconds\")\n",
    "print(\"score of NN on train: \"+ str(model.evaluate(X_train,y_train)[1]))\n",
    "print(\"score of NN on test: \" + str(model.evaluate(X_test,y_test)[1]))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights= model.layers[0].get_weights()[0]\n",
    "weights = weights.reshape(-1)\n",
    "#print(weights)\n",
    "plt.hist(weights)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "# model.save(\"combined_data_stuff/nn_weights.h5\")\n",
    "clf = DecisionTreeClassifier()\n",
    "st = time.time()\n",
    "clf = clf.fit(x_train_data,y_train_data)\n",
    "end = time.time()\n",
    "time_taken_dt_train = end-st\n",
    "print(\"Time taken to train DT = \", time_taken_dt_train, \"seconds\")\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "print(\"score of DT on train: \"+ str(accuracy_score(y_pred_train,y_train)))\n",
    "print(\"score of DT on test: \" + str(accuracy_score(y_pred_test,y_test)))\n",
    "model_2=models.Sequential()\n",
    "model_2.add(layers.Dense(x_train_data.shape[1],activation='relu',input_shape=(x_train_data.shape[1],)))\n",
    "model_2.add(layers.Dense(1024,activation='relu'))\n",
    "model_2.add(layers.Dense(256,activation='relu'))\n",
    "model_2.add(layers.Dense(64,activation='relu'))\n",
    "model_2.add(layers.Dense(1,activation='sigmoid'))\n",
    "model_2.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "model_2 = load_model('combined_data_stuff/nn_weights.h5')\n",
    "print(\"score of NN on train: \"+ str(model_2.evaluate(X_train,y_train)[1]))\n",
    "print(\"score of NN on test: \" + str(model_2.evaluate(X_test,y_test)[1]))\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train_data.values,\n",
    "                                                   feature_names=x_train_data.columns,\n",
    "                                                   class_names=['Non-PII', 'PII'],\n",
    "                                                   discretize_continuous=True)\n",
    "\n",
    "instance_idx = 0\n",
    "\n",
    "def predict_proba_wrapper(X):\n",
    "    # Make sure to modify 'model' to your TensorFlow Sequential model\n",
    "    preds = model.predict(X)\n",
    "    # If only one class is present in the data, create a placeholder probability for the missing class\n",
    "    if preds.shape[1] == 1:\n",
    "        preds = np.concatenate([preds, 1 - preds], axis=1)\n",
    "    return preds\n",
    "\n",
    "explanation = explainer.explain_instance(x_val_data.values[instance_idx],\n",
    "                                         predict_proba_wrapper, num_features=len(x_train_data.columns))\n",
    "\n",
    "\n",
    "#fig = explanation.as_pyplot_figure()\n",
    "pd.DataFrame(explanation.as_list())\n",
    "\n",
    "\n",
    "# Display the figure\n",
    "\n",
    "\n",
    "#explanation.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(x_train_data.values,\n",
    "                                                   feature_names=x_train_data.columns,\n",
    "                                                   class_names=['Non-PII', 'PII'],\n",
    "                                                   discretize_continuous=True)\n",
    "\n",
    "instance_idx = 0\n",
    "\n",
    "def predict_proba_wrapper(X):\n",
    "    # Make sure to modify 'model' to your TensorFlow Sequential model\n",
    "    preds = model.predict(X)\n",
    "    # If only one class is present in the data, create a placeholder probability for the missing class\n",
    "    if preds.shape[1] == 1:\n",
    "        preds = np.concatenate([preds, 1 - preds], axis=1)\n",
    "    return preds\n",
    "\n",
    "explanation = explainer.explain_instance(x_val_data.values[instance_idx],\n",
    "                                         predict_proba_wrapper, num_features=len(x_train_data.columns))\n",
    "\n",
    "\n",
    "#fig = explanation.as_pyplot_figure()\n",
    "pd.DataFrame(explanation.as_list())\n",
    "\n",
    "\n",
    "# Display the figure\n",
    "\n",
    "\n",
    "#explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Convert validation data to numpy array\n",
    "x_test_np = X_test.to_numpy()\n",
    "\n",
    "# Create the SHAP explainer using TFDeepExplainer and a random sample of the validation data as the background dataset\n",
    "background_data_sample = x_test_np[np.random.choice(x_test_np.shape[0], 100, replace=False)]\n",
    "explainer = shap.DeepExplainer(model, background_data_sample)\n",
    "\n",
    "# Compute SHAP values for the validation data\n",
    "shap_values = explainer.shap_values(x_test_np)\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "shap.summary_plot(shap_values, x_test_np, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Convert validation data to numpy array\n",
    "x_test_np = X_test.to_numpy()\n",
    "\n",
    "# Create the SHAP explainer using TFDeepExplainer and a random sample of the validation data as the background dataset\n",
    "background_data_sample = x_test_np[np.random.choice(x_test_np.shape[0], 100, replace=False)]\n",
    "explainer = shap.DeepExplainer(model, background_data_sample)\n",
    "\n",
    "# Compute SHAP values for the validation data\n",
    "shap_values = explainer.shap_values(x_test_np)\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "shap.summary_plot(shap_values, x_test_np, plot_type=\"bar\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
